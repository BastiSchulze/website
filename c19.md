---
layout: teaching
title: Machine Learning
tags: teaching
---
Machine learning is a rapidly developing field that lies at the intersection of statistics and computer science.
Its growth can be partially attributed to the vast quantities of data (known as "big data") that are now routinely being captured in science and industry.
Machine learning aims to find means of taking decisions upon data that are both computationally tractable and statistically principled.
This course will briefly introduce core concepts in machine learning: supervised vs unsupervised learning; nearest neighbour methods; the challenge of generalisation; and approximate inference (in the form of Laplace's method and variational methods).
The course is taught by Michael A Osborne, incorporating material prepared by [Andrew Zisserman.](http://www.robots.ox.ac.uk/~az/)


## Course Materials

### Lectures
NB: these topics are intended to follow on from [B14: Estimation and Inference]({{ site.baseurl }}/b14) and [C24: Advanced Probability Theory]({{ site.baseurl }}/c24).

<ul class='plus'>
    <li>
        <a href="{{ site.baseurl }}/teaching/C19/1_introduction.pdf">
            topic 1 handouts: introduction to machine learning
        </a>
    </li>
    <li>
        <a href="{{ site.baseurl }}/teaching/C19/2_inference_is_integration.pdf">
            topic 2 handouts: inference is integration
        </a>
    </li>
</ul>

### Examples Class

The solutions to the example sheet will be provided below after the classes: of course, you will benefit most from attempting the questions on your own first.

<ul class='plus'>
    <li>
        <a href="{{ site.baseurl }}/teaching/C19/c19_adv_prob_questions.pdf">
            example sheet
        </a>
    </li>    
    <li>
        <a href="{{ site.baseurl }}/teaching/C19/c19_adv_prob_solutions.pdf">
            solutions
        </a>
    </li>
</ul>

<!-- 
## Past papers

Please note that the solutions to the probability questions on 2012 and 2014 past C24 papers are slightly incorrect. You may wish to refer instead to problems from the texts below. 
 -->

## Core reading

<ul class='plus'>
<li>K. P. Murphy, <a href="http://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_1?ie=UTF8&qid=1452883070&sr=8-1&keywords=machine+learning+a+probabilistic+perspective">'Machine Learning: A Probabilistic Perspective'</a>, 2012, MIT Press.</li>
<li>David Barber, <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online"> Bayesian Reasoning and Machine Learning</a>', (pdf available for free), 2012, Cambridge University Press. </li>
<li>David MacKay, <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.html"> Information Theory, Inference, and Learning Algorithms</a>' (pdf available for free), 2003, Cambridge University Press. 
</li>
<li>
    Shakir Mohamed, <a href="http://www.shakirm.com/papers/VITutorial.pdf">A Tutorial on Variational Inference for Machine Learning.</a> 2015.
</li>
</ul>

