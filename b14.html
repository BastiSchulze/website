---
layout: hidden_page
title: B14 Estimation and Inference
---

<p>This course is taught by <a href="index.html">Michael A Osborne,</a> based on that devised by Prof David Murray.</p>
<br>
<h2>Course Materials</h2>
<h3>Lectures</h3>
<ul class='plus'>
<li><a href="{{ site.baseurl }}teaching/B14/1_slides_foundations.pdf">topic 1 handouts: foundations of probability</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/2_slides_continuous.pdf">topic 2 handouts: probability of continuous variables</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/3_slides_Gaussian.pdf">topic 3 handouts: the gaussian distribution: the best distribution</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/Gaussian_identities.pdf">gaussian identities cheat sheet</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/4_slides_ML.pdf">topic 4 handouts: maximum likelihood estimation</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/5_slides_MAP.pdf">topic 5 handouts: maximum a posteriori estimation</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/6_slides_filtering.pdf">topic 6 handouts: temporal filtering</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/7_slides_decisions.pdf">topic 7 handouts: decision theory and classification</a></li>
</ul>
<h3>FAQ</h3>
<p><a href="{{ site.baseurl }}teaching/B14/b14_EI_FAQ.pdf">FAQ</a></p>
<!-- <p><a href="{{ site.baseurl }}teaching/B14/slides_summary.pdf">topic 8 handouts: summary</a></p> -->
<h3>Classes</h3>
<ul class='plus'>
<li><a href="{{ site.baseurl }}teaching/B14/B14-1Questions.pdf">question sheet 1</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/B14-1Hints.pdf">question sheet 1 hints</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/B14-2Questions.pdf">question sheet 2</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/B14-2Hints.pdf">question sheet 2 hints</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/B14-2q1.m">matlab for question sheet 2 q 1</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/b14_ekf.m">matlab for question sheet 2 q 3e</a></li>
<li><a href="{{ site.baseurl }}teaching/B14/b14_ekf_draws.m">matlab for question sheet 2 q 3f</a></li>
</ul>
<h2>Course Outline</h2>

<p>
  Basics of pdfs: properties, joints, conditional and marginalisation,
  combining pdfs (convolutions, multiplications). Parametric representation of
  pdfs: multi-variate Gaussian distribution and its properties, moments,
  correlation. Central limit theorem. Belief Networks. Modelling sensors
  probabilistically; Bayes rule. Estimators: ML, MAP and Bayes, bias and
  variance, applications to model fitting. Recursive estimation; Decisions and
  classification: linear classifier, Jacobian for prop. uncertainty.
</p>

<p>After this course the student will have an understanding of the following, all in the context of the interpretation of sensor data:
</p>
<ul class='plus'>
<li>Basic properties of distributions: independence, joint and conditional distributions, marginals. Bayes' theorem. The extension from univariate to multivariate distributions.</li>
<li>How to represent distributions parametrically, how to transform into new sets of random variables, and how to combine density functions.</li>
<li>The representation of a distribution as a belief network.</li>
<li>The multivariate Gaussian distribution and its properties.</li>
<li>How to devise generative models and the optimization of their parameters using Maximum Likelihood Estimation.</li>
<li>The relationship between MLE and Least Squares optimization.</li>
<li>The use of priors and Bayes' rule in Maximum A Posteriori estimation.</li>
<li>Sequential inference, in particular, Kalman filtering.</li>
<li>Bayesian Decision Theory: the specification of appropriate loss functions, and the minimisation of expected loss to select actions.</li>
<li>Classifiers and Decision Surfaces, particularly the discriminant function derived from Normal distributions.</li>
<li>Linear Classifiers - decision hyperplanes and their appearance in Logistic Regression.</li>
<li>Shortcomings of linear methods, and possibilities for non-linear classification.</li>
</ul>

<h2>Core reading</h2>


<ul class='plus'>
<li>David Barber, <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online"> Bayesian Reasoning and Machine Learning</a>', (pdf available for free), 2012, Cambridge University Press. Chapters: 1; 3.1, 3.3; 7.1-7.2; 8; 9.1-9.2; 10; 17.1-17.4; 24.1-24.4.</li>
<li>Phil Gregory, 'Bayesian Logical Data Analysis for the Physical Sciences&rsquo;, 2010, Cambridge University Press Chapters: 1; 3.1-3.4; 5.1-5.12; 9.</li>
<li>David MacKay, <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.html"> Information Theory, Inference, and Learning Algorithms</a>' (pdf available for free), 2003, Cambridge University Press. Chapters: 2.1-2.4; 3; 36.</li>
</ul>

<h2>Other recommended texts</h2>

<ul class='plus'>
<li>D. S. Sivia, 'Data Analysis: A Bayesian Tutorial', 2006, Oxford University Press</li>
<li>C. M. Bishop, 'Pattern Recognition and Machine Learning', 2006, Springer</li>
<li>K. P. Murphy, 'Machine Learning: A Probabilistic Perspective', 2012, MIT Press</li>
</ul>

<h2>Exam revision</h2>
<p> Note that in previous exam papers, "information" is used as a synonym for "precision".</p>

<h3>tute-sheets</h3>
<p>all questions are of appropriate difficulty, although not necessarily
of appropriate length, for exam practice.</p>

<h3>b4</h3>
<ul class='plus'>
<li>2005: q4, q8</li>
<li>2006: q4, q5</li>
<li>2007: q2, q3</li>
<li>2008: q6</li>
<li>2009: q5</li>
<li>2010: q6</li>
</ul>

<h3>b14</h3>
<ul class='plus'>
<li>2011: q1, q2</li>
<li>2012: q3, q4</li>
<li>2013: q3, q4</li>
<li>2014: q3, q4</li>
</ul>
