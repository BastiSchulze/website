---
layout: teaching
title: B14 Inference
tags: teaching
---

<p>This course follows on from <a href="http://www.robots.ox.ac.uk/~mfallon/teaching/">B14 Estimation</a>.
It provides an introduction to probabilistic inference, covering maximum likelihood estimation and comparing against Bayesian inference.
The course concludes by introducing decision theory and classification. The course is taught by <a href="index.html">Michael A Osborne,</a> based on that devised by Prof David Murray.</p>
<br>
<h2>Course Materials</h2>
<h3>Lectures</h3>
<ul class='plus'>
<li><a href="{{ site.baseurl }}/teaching/B14/4_slides_ML.pdf">topic 4 handouts: maximum likelihood estimation</a></li>
<li><a href="{{ site.baseurl }}/teaching/B14/5_slides_MAP.pdf">topic 5 handouts: maximum a posteriori estimation</a></li>
<li><a href="{{ site.baseurl }}/teaching/B14/6_slides_decisions.pdf">topic 6 handouts: decision theory and classification</a></li>
</ul>
<h3>FAQ</h3>
<p><a href="{{ site.baseurl }}/teaching/B14/b14_EI_FAQ.pdf">FAQ</a></p>
<!-- <p><a href="{{ site.baseurl }}/teaching/B14/slides_summary.pdf">topic 8 handouts: summary</a></p> -->
<h3>Classes</h3>
<ul class='plus'>
<li><a href="{{ site.baseurl }}/teaching/B14/B14-2Questions.pdf">question sheet</a></li>
<li><a href="{{ site.baseurl }}/teaching/B14/B14-2Hints.pdf">question sheet hints</a></li>
<li><a href="{{ site.baseurl }}/teaching/B14/B14-2q1.m">matlab for question sheet q 1</a></li>
<li><a href="{{ site.baseurl }}/teaching/B14/B14-2Solutions.pdf">question sheet solutions</a></li>
</ul>
<h2>Course Outline</h2>

<p>After this course the student will have an understanding of the following, all in the context of the interpretation of sensor data:
</p>
<ul class='plus'>
    <li>How to devise generative models and the optimization of their parameters using Maximum Likelihood Estimation.</li>
    <li>The relationship between MLE and Least Squares optimization.</li>
    <li>The use of priors and Bayes' rule in Maximum A Posteriori estimation.</li>
    <li>Sequential inference, in particular, Kalman filtering.</li>
    <li>Bayesian Decision Theory: the specification of appropriate loss functions, and the minimisation of expected loss to select actions.</li>
    <li>Classifiers and Decision Surfaces, particularly the discriminant function derived from Normal distributions.</li>
    <li>Linear Classifiers â€“ decision hyperplanes and their appearance in Logistic Regression.</li>
    <li>Shortcomings of linear methods, and possibilities for non-linear classification.</li>
</ul>

<h2>Core reading</h2>


<ul class='plus'>
<li>David Barber, <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online"> Bayesian Reasoning and Machine Learning</a>', (pdf available for free), 2012, Cambridge University Press. Chapters: 1; 3.1, 3.3; 7.1-7.2; 8; 9.1-9.2; 10; 17.1-17.4; 24.1-24.4.</li>
<li>Phil Gregory, 'Bayesian Logical Data Analysis for the Physical Sciences&rsquo;, 2010, Cambridge University Press Chapters: 1; 3.1-3.4; 5.1-5.12; 9.</li>
<li>David MacKay, <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.html"> Information Theory, Inference, and Learning Algorithms</a>' (pdf available for free), 2003, Cambridge University Press. Chapters: 2.1-2.4; 3; 36.</li>
</ul>

<h2>Other recommended texts</h2>

<ul class='plus'>
<li>D. S. Sivia, 'Data Analysis: A Bayesian Tutorial', 2006, Oxford University Press</li>
<li>C. M. Bishop, 'Pattern Recognition and Machine Learning', 2006, Springer</li>
<li>K. P. Murphy, 'Machine Learning: A Probabilistic Perspective', 2012, MIT Press</li>
</ul>

<h2>Exam revision</h2>
<p> Note that in previous exam papers, "information" is used as a synonym for "precision".</p>

<h3>tute-sheets</h3>
<p>all questions are of appropriate difficulty, although not necessarily
of appropriate length, for exam practice.</p>

<h3>b4</h3>
<ul class='plus'>
<li>2005: q4, q8</li>
<li>2006: q4, q5</li>
<li>2007: q2, q3</li>
<li>2008: q6</li>
<li>2009: q5</li>
<li>2010: q6</li>
</ul>

<h3>b14</h3>
<ul class='plus'>
<li>2011: q1, q2</li>
<li>2012: q3, q4</li>
<li>2013: q3, q4</li>
<li>2014: q3, q4</li>
<li>2015: q3, q4</li>
<li>2016: q3, q4</li>
<li>2017: q3, q4</li>
</ul>
