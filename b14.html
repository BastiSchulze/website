<html>
<head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-27639330-3', 'ox.ac.uk');
  ga('send', 'pageview');

</script>
<title>Michael A Osborne</title>
<script type="text/javascript">
  (function(d) {
    var config = {
      kitId: 'flp8eda',
      scriptTimeout: 3000
    },
    h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='//use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)
  })(document);
</script>
<link rel="stylesheet" type="text/css" href="style.css" />
<link rel="shortcut icon" type="image/x-icon" href="http://www.eng.ox.ac.uk/favicon.ico" />
</script>
 </head>


<div id="container">

<div id="header">
 <h1>B14: Estimation and Inference</h1>
</div>
<br>
<p id="first">This course is taught by <a href="index.html">Michael A Osborne,</a> based on that devised by Prof David Murray.</p>
<br>
<h3>Course Materials</h3>
<h4>Lectures</h4>
<p><a href="./teaching/B14/1_slides_foundations.pdf">topic 1 handouts: foundations of probability</a></p>
<p><a href="./teaching/B14/2_slides_continuous.pdf">topic 2 handouts: probability of continuous variables</a></p>
<p><a href="./teaching/B14/3_slides_Gaussian.pdf">topic 3 handouts: the gaussian distribution: the best distribution</a></p>
<p><a href="./teaching/B14/Gaussian_identities.pdf">gaussian identities cheat sheet</a></p>
<p><a href="./teaching/B14/4_slides_ML.pdf">topic 4 handouts: maximum likelihood estimation</a></p>
<p><a href="./teaching/B14/5_slides_MAP.pdf">topic 5 handouts: maximum a posteriori estimation</a></p>
<p><a href="./teaching/B14/6_slides_filtering.pdf">topic 6 handouts: temporal filtering</a></p>
<p><a href="./teaching/B14/7_slides_decisions.pdf">topic 7 handouts: decision theory and classification</a></p>
<h4>FAQ</h4>
<p><a href="teaching/B14/b14_EI_FAQ.pdf">FAQ</a></p>
<!-- <p><a href="./teaching/B14/slides_summary.pdf">topic 8 handouts: summary</a></p> -->
<h4>Classes</h4>
<p><a href="./teaching/B14/B14-1Questions.pdf">question sheet 1</a></p>
<p><a href="./teaching/B14/B14-1Hints.pdf">question sheet 1 hints</a></p>
<p><a href="./teaching/B14/B14-2Questions.pdf">question sheet 2</a></p>
<p><a href="./teaching/B14/B14-2Hints.pdf">question sheet 2 hints</a></p>
<p><a href="./teaching/B14/B14-2q1.m">matlab for question sheet 2 q 1</a></p>
<p><a href="./teaching/B14/b14_ekf.m">matlab for question sheet 2 q 3e</a></p>
<p><a href="./teaching/B14/b14_ekf_draws.m">matlab for question sheet 2 q 3f</a></p>

<h3>Course Outline</h3>

<p id="first">
  Basics of pdfs: properties, joints, conditional and marginalisation,
  combining pdfs (convolutions, multiplications). Parametric representation of
  pdfs: multi-variate Gaussian distribution and its properties, moments,
  correlation. Central limit theorem. Belief Networks. Modelling sensors
  probabilistically; Bayes rule. Estimators: ML, MAP and Bayes, bias and
  variance, applications to model fitting. Recursive estimation; Decisions and
  classification: linear classifier, Jacobian for prop. uncertainty.
</p>

<p>After this course the student will have an understanding of the following, all in the context of the interpretation of sensor data:
</p>
<ol>
<li>Basic properties of distributions: independence, joint and conditional distributions, marginals. Bayes' theorem. The extension from univariate to multivariate distributions.</li>
<li>How to represent distributions parametrically, how to transform into new sets of random variables, and how to combine density functions.</li>
<li>The representation of a distribution as a belief network.</li>
<li>The multivariate Gaussian distribution and its properties.</li>
<li>How to devise generative models and the optimization of their parameters using Maximum Likelihood Estimation.</li>
<li>The relationship between MLE and Least Squares optimization.</li>
<li>The use of priors and Bayes' rule in Maximum A Posteriori estimation.</li>
<li>Sequential inference, in particular, Kalman filtering.</li>
<li>Bayesian Decision Theory: the specification of appropriate loss functions, and the minimisation of expected loss to select actions.</li>
<li>Classifiers and Decision Surfaces, particularly the discriminant function derived from Normal distributions.</li>
<li>Linear Classifiers - decision hyperplanes and their appearance in Logistic Regression.</li>
<li>Shortcomings of linear methods, and possibilities for non-linear classification.</li>
</ol>

<h3>Core reading</h3>
<p class="c1"><span>David Barber,
</span><span class="c16 c10">
<a class="c0" href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online">
Bayesian Reasoning and Machine Learning</a></span><span>',
(pdf available for free), 2012, Cambridge University Press.</span></p>
<p class="c1"><span>Chapters: 1; 3.1, 3.3; 7.1-7.2; 8; 9.1-9.2; 10; 17.1-17.4; 24.1-24.4.</span></p>
<p class="c1"><span>Phil Gregory, `Bayesian Logical Data Analysis for the
Physical Sciences&rsquo;, 2010, Cambridge University Press</span></p>
<p class="c1"><span>Chapters: 1; 3.1-3.4; 5.1-5.12; 9.</span></p>
<p class="c1"><span>David MacKay, '</span><span class="c10 c16">
<a class="c0" href="http://www.inference.phy.cam.ac.uk/itprnn/book.html">
Information Theory, Inference, and Learning Algorithms</a></span><span>'
(pdf available for free), 2003, Cambridge University Press.
</span></p><p class="c1"><span>Chapters: 2.1-2.4; 3; 36.
</span></p><p class="c1 c3"><span></span></p><p class="c1">
<h3>Other recommended texts:</h3></p><p class="c1">
<span>D. S. Sivia, 'Data Analysis: A Bayesian Tutorial', 2006,
Oxford University Press</span></p><p class="c1">
<span>C. M. Bishop, 'Pattern Recognition and Machine Learning',
2006, Springer</span></p><p class="c1"><span>K. P. Murphy,
'Machine Learning: A Probabilistic Perspective', 2012, MIT Press</span></p>

<h3>Exam revision</h3>
<p id="first"> Note that in previous exam papers, "information" is used as a synonym for "precision".</p>

<h4>tute-sheets</h4>
<p id="first">all questions are of appropriate difficulty, although not necessarily
of appropriate length, for exam practice.</p>

<h4>b4</h4>
<p>2005: q4, q8</p>
<p>2006: q4, q5</p>
<p>2007: q2, q3</p>
<p>2008: q6</p>
<p>2009: q5</p>
<p>2010: q6</p>

<h4>b14</h4>
<p>2011: q1, q2</p>
<p>2012: q3, q4</p>
<p>2013: q3, q4</p>

</div>



<!--
<br>
<div id="thinDivider"></div>
<p>
<a name="ycbm">DPhil students, you may wish to book my time using this:</a>
<iframe src="http://mosb.youcanbook.me/" style="width:100%;height:400px;border:0px;background-color:transparent;" frameborder="0" allowtransparency="true"></iframe>
-->
</html>
