
@article{rajpaul_gaussian_2015,
	title = {A {Gaussian} process framework for modelling stellar activity signals in radial velocity data},
	volume = {452},
	issn = {0035-8711, 1365-2966},
	url = {http://mnras.oxfordjournals.org/content/452/3/2269},
	doi = {10.1093/mnras/stv1428},
	abstract = {To date, the radial velocity (RV) method has been one of the most productive techniques for detecting and confirming extrasolar planetary candidates. Unfortunately, stellar activity can induce RV variations which can drown out or even mimic planetary signals – and it is notoriously difficult to model and thus mitigate the effects of these activity-induced nuisance signals. This is expected to be a major obstacle to using next-generation spectrographs to detect lower mass planets, planets with longer periods, and planets around more active stars. Enter Gaussian processes (GPs) which, we note, have a number of attractive features that make them very well suited to disentangling stellar activity signals from planetary signals. We present here a GP framework we developed to model RV time series jointly with ancillary activity indicators (e.g. bisector velocity spans, line widths, chromospheric activity indices), allowing the activity component of RV time series to be constrained and disentangled from e.g. planetary components. We discuss the mathematical details of our GP framework, and present results illustrating its encouraging performance on both synthetic and real RV data sets, including the publicly available Alpha Centauri B data set.},
	language = {en},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Rajpaul, Vinesh and Aigrain, Suzanne and Osborne, Michael A. and Reece, Steve and Roberts, Stephen J.},
	month = sep,
	year = {2015},
	keywords = {methods: data analysis, planetary systems, stars: activity, stars: individual: Alpha Centauri B, stars: individual: Gliese 15 A, techniques: radial velocities},
	pages = {2269--2291},
	file = {1513/1506.07304v1.pdf}
}

@article{hennig_probabilistic_2015,
	title = {Probabilistic {Numerics} and {Uncertainty} in {Computations}},
	volume = {471},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/content/royprsa/471/2179/20150142.full.pdf?ijkey=wr6Ggr6GGGgbJYr&keytype=ref},
	doi = {10.1098/rspa.2015.0142},
	abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
	number = {2179},
	journal = {Proceedings of the Royal Society A},
	author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark A.},
	year = {2015},
	file = {1883/Hen_Osb_Gir_RSPA_2015.pdf}
}

@article{garnett_sequential_2010,
	title = {Sequential {Bayesian} prediction in the presence of changepoints and faults},
	volume = {53},
	doi = {doi:10.1093/comjnl/bxq003},
	abstract = {We introduce a new sequential algorithm for making robust predictions in the presence of changepoints. Unlike previous approaches, which focus on the problem of detecting and locating changepoints, our algorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary covariance functions to be used in Gaussian process prediction that model such changes, and then proceed to demonstrate how to effectively manage the hyperparameters associated with those covariance functions. We further introduce covariance functions to be used in situations where our observation model undergoes changes, as is the case for sensor faults. By using Bayesian quadrature, we can integrate out the hyperparameters, allowing us to calculate the full marginal predictive distribution. Furthermore, if desired, the posterior distribution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.},
	number = {9},
	journal = {The Computer Journal},
	author = {Garnett, Roman and Osborne, Michael A. and Reece, Steven and Rogers, Alex and Roberts, Stephen J.},
	year = {2010},
	pages = {1430},
	file = {1574/journal_changept.pdf}
}

@article{luca_photodegradation_2006,
	title = {Photodegradation of methylene blue using crystalline titanosilicate quantum-confined semiconductor},
	volume = {18},
	url = {http://pubs.acs.org/doi/abs/10.1021/cm052839p},
	doi = {10.1021/cm052839p},
	abstract = {Synthetic sitinakite contains in its structure a discrete wire-like sublattice of linked TiO6 octahedra. This sublattice is held apart by silicate tetrahedra forming one-dimensional channels that run down the c axis. The optical properties of this structural arrangement have been studied and compared with other titanosilicate phases, the best known being ETS-10. Thus, sitinakite which has twice the titanate wire diameter of ETS-10 has a band gap of 4.07 eV compared with 3.87 eV. The reduced electron−hole effective mass of the sitinakite quantum-confined system has been calculated through use of the effective mass model and compared with that of other titanosilicate materials. The sitinakite phase has been shown to effectively photodegrade methylene blue (MB) dye at pH 7 using visible light excitation and displays a higher degradation rate than TiO2 (Degussa, P25) under the same experimental conditions. On the contrary, under UV excitation, the photodegradation rate obtained using P25 is much higher than that using sitinakite. Given that the band edge of sitinkaite is significantly blue shifted compared with that of P25, photodegradation of MB using sitinakite is attributed to sensitization of the MB cationic dye which is strongly adsorbed onto the negatively charged sitinakite surfaces.},
	number = {26},
	journal = {Chemistry of materials},
	author = {Luca, Vittorio and Osborne, Michael A. and Sizgek, Devlet and Griffith, Christopher and Araujo, Paula Z.},
	year = {2006},
	pages = {6132--6138}
}

@article{gibson_gaussian_2012,
	title = {A {Gaussian} process framework for modelling instrumental systematics: application to transmission spectroscopy},
	volume = {419},
	doi = {10.1111/j.1365-2966.2011.19915.x},
	abstract = {Transmission spectroscopy, which consists of measuring the wavelength-dependent absorption of starlight by a planet’s atmosphere during a transit, is a powerful probe of atmospheric composition. However, the expected signal is typically orders of magnitude smaller than instrumental systematics, and the results are crucially dependent on the treatment of the latter. In this paper, we propose a new method to infer transit parameters in the presence of systematic noise using Gaussian processes, a technique widely used in the machine learning community for Bayesian regression and classification problems. Our method makes use of auxiliary information about the state of the instrument, but does so in a non-parametric manner, without imposing a specific dependence of the systematics on the instrumental parameters, and naturally allows for the correlated nature of the noise. We give an example application of the method to archival NICMOS transmission spectroscopy of the hot Jupiter HD 189733, which goes some way towards reconciling the controversy surrounding this dataset in the literature. Finally, we provide an appendix giving a general introduction to Gaussian processes for regression, in order to encourage their application to a wider range of problems.},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Gibson, Neale P. and Aigrain, Suzanne and Roberts, Stephen J. and Evans, Tom and Osborne, Michael A. and Pont, Frederic},
	year = {2012},
	pages = {2683--2694},
	file = {931/Gibson2011instrumental_systematics.pdf}
}

@article{ng_using_2014,
	title = {Using textons to rank crystallization droplets by the likely presence of crystals},
	volume = {70},
	issn = {1399-0047},
	url = {http://scripts.iucr.org/cgi-bin/paper?S1399004714017581},
	doi = {10.1107/S1399004714017581},
	abstract = {The visual inspection of crystallization experiments is an important yet time-consuming and subjective step in X-ray crystallography. Previously published studies have focused on automatically classifying crystallization droplets into distinct but ultimately arbitrary experiment outcomes; here, a method is described that instead ranks droplets by their likelihood of containing crystals or microcrystals, thereby prioritizing for visual inspection those images that are most likely to contain useful information. The use of textons is introduced to describe crystallization droplets objectively, allowing them to be scored with the posterior probability of a random forest classifier trained against droplets manually annotated for the presence or absence of crystals or microcrystals. Unlike multi- class classification, this two-class system lends itself naturally to unidirectional ranking, which is most useful for assisting sequential viewing because images can be arranged simply by using these scores: this places droplets with probable crystal- line behaviour early in the viewing order. Using this approach, the top ten wells included at least one human-annotated crystal or microcrystal for 94\% of the plates in a data set of 196 plates imaged with a Minstrel HT system. The algorithm is robustly transferable to at least one other imaging system: when the parameters trained from Minstrel HT images are applied to a data set imaged by the Rock Imager system, human-annotated crystals ranked in the top ten wells for 90\% of the plates. Because rearranging images is fundamental to the approach, a custom viewer was written to seamlessly support such ranked viewing, along with another important output of the algorithm, namely the shape of the curve of scores, which is itself a useful overview of the behaviour of the plate; additional features with known usefulness were adopted from existing viewers. Evidence is presented that such ranked viewing of images allows faster but more accurate evaluation of drops, in particular for the identification of microcrystals.},
	number = {10},
	journal = {Acta Crystallographica Section D Biological Crystallography},
	author = {Ng, Jia Tsing and Dekker, Carien and Kroemer, Markus and Osborne, Michael A. and von Delft, Frank},
	month = oct,
	year = {2014},
	file = {1138/nj5198.pdf}
}

@article{richardson_gaussian_2017,
	title = {Gaussian process regression for forecasting battery state of health},
	volume = {357},
	issn = {0378-7753},
	url = {http://www.sciencedirect.com/science/article/pii/S0378775317306250},
	doi = {10.1016/j.jpowsour.2017.05.004},
	abstract = {Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.},
	journal = {Journal of Power Sources},
	author = {Richardson, Robert R. and Osborne, Michael A. and Howey, David A.},
	month = jul,
	year = {2017},
	keywords = {Ageing, Gaussian process regression, Lithium-ion battery, Prognostics, State-of-health},
	pages = {209--219},
	file = {926/Richardson et al. - 2017 - Gaussian process regression for forecasting batter.pdf}
}

@article{osborne_real-time_2012,
	title = {Real-{Time} {Information} {Processing} of {Environmental} {Sensor} {Network} {Data}},
	volume = {9},
	doi = {10.1145/2379799.2379800},
	abstract = {In this paper, we consider the problem faced by a sensor network operator who must infer, in real-time, the value of some environmental parameter that is being monitored at discrete points in space and time by a sensor network. We describe a powerful and generic approach built upon an efficient multi-output Gaussian process that facilitates this information acquisition and processing. Our algorithm allows effective inference even with minimal domain knowledge, and we further introduce a formulation of Bayesian Monte Carlo to permit the principled management of the hyperparameters introduced by our flexible models. We demonstrate how our methods can be applied in cases where the data is delayed, intermittently missing, censored and/or correlated. We validate our approach using data collected from three networks of weather sensors and show that it yields better inference performance than both conventional independent Gaussian processes and the Kalman filter. Finally, we show that our formalism efficiently re-uses previous computations by following an online update procedure as new data sequentially arrives, and that this results in a four-fold increase in computational speed in the largest cases considered.},
	number = {1},
	journal = {ACM Transactions on Sensor Networks},
	author = {Osborne, Michael A. and Roberts, Stephen J. and Rogers, Alex and Jennings, Nicholas R.},
	year = {2012},
	note = {00000},
	pages = {1:1--1:32},
	file = {1240/tosn_gp_revised.pdf}
}

@article{roberts_gaussian_2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
	number = {1984},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, Stephen J. and Osborne, Michael A. and Ebden, Mark and Reece, Steve and Gibson, Neale P. and Aigrain, Suzanne},
	year = {2013},
	pages = {20110550},
	file = {2151/philTransA_2012.pdf}
}

@article{mann_objectively_2011,
	title = {Objectively identifying landmark use and predicting flight trajectories of the homing pigeon using {Gaussian} processes},
	volume = {8},
	doi = {10.1098/rsif.2010.0301},
	abstract = {Pigeons home along idiosyncratic habitual routes from familiar locations. It has been suggested that memorized visual landmarks underpin this route learning. However, the inability to experimentally alter the landscape on large scales has hindered the discovery of the particular features to which birds attend. Here, we present a method for objectively classifying the most informative regions of animal paths. We apply this method to flight trajectories from homing pigeons to identify probable locations of salient visual landmarks. We construct and apply a Gaussian process model of flight trajectory generation for pigeons trained to home from specific release sites. The model shows increasing predictive power as the birds become familiar with the sites, mirroring the animal’s learning process. We subsequently find that the most informative elements of the flight trajectories coincide with landscape features that have previously been suggested as important components of the homing task.},
	number = {55},
	journal = {Journal of The Royal Society Interface},
	author = {Mann, Richard and Freeman, Robin and Osborne, Michael A. and Garnett, Roman and Armstrong, Chris and Meade, Jessica and Biro, Dora and Guilford, Tim and Roberts, Stephen J.},
	year = {2011},
	pages = {210--219},
	file = {1464/J. R. Soc. Interface-2011-Mann-210-9.pdf}
}

@article{frey_future_2017,
	title = {The future of employment: {How} susceptible are jobs to computerisation?},
	volume = {114},
	issn = {0040-1625},
	shorttitle = {The future of employment},
	url = {http://www.sciencedirect.com/science/article/pii/S0040162516302244},
	doi = {10.1016/j.techfore.2016.08.019},
	abstract = {We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classifier. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupations probability of computerisation, wages and educational attainment.},
	journal = {Technological Forecasting and Social Change},
	author = {Frey, Carl Benedikt and Osborne, Michael A.},
	month = jan,
	year = {2017},
	keywords = {Employment, Occupational choice, Skill demand, Technological change, wage inequality},
	pages = {254--280},
	file = {2284/Frey and Osborne - 2017 - The future of employment How susceptible are jobs.pdf}
}
@inproceedings{mann_gaussian_2009,
	title = {Gaussian {Processes} for {Prediction} of {Homing} {Pigeon} {Flight} {Trajectories}},
	volume = {1193},
	doi = {10.1063/1.3275635},
	abstract = {We construct and apply a stochastic Gaussian Process (GP) model of flight trajectory generation for pigeons trained to home from specific release sites. The model shows increasing pre- dictive power as the birds become familiar with the sites, mirroring the animal’s learning process. We show how the increasing similarity between successive flight trajectories can be used to infer, with increasing accuracy, an idealised route that captures the repeated spatial aspects of the bird’s flight. We subsequently use techniques associated with reduced-rank GP approximations to objec- tively identify the key waypoints used by each bird to memorise its idiosyncratic habitual route between the release site and the home loft.},
	booktitle = {{AIP} {Conference} {Proceedings}},
	author = {Mann, Richard and Freeman, Robin and Osborne, Michael A. and Garnett, Roman and Meade, Jessica and Armstrong, Chris and Biro, Dora and Guilford, Tim and Roberts, Stephen J. and Goggans, Paul M.},
	year = {2009},
	pages = {360},
	file = {1281/mann_maxent_final.pdf}
}

@inproceedings{calliess_conservative_2014,
	title = {Conservative collision prediction and avoidance for stochastic trajectories in continuous time and space},
	isbn = {1-4503-2738-9},
	abstract = {Existing work in multi-agent collision prediction and avoidance typically assumes discrete-time tra jectories with Gaussian uncertainty or that are completely deterministic. We propose an approach that allows detection of collisions even between continuous, stochastic trajectories with the only restriction that means and covariances can be computed. To this end, we employ probabilistic bounds to derive criterion functions whose nega tive sign provably is indicative of probable colli sions. For criterion functions that are Lipschitz, an algorithm is provided to rapidly find negative values or prove their absence. We propose an iterative policy-search approach that avoids prior discretisations and yields collision-free trajectories with adjustably high certainty. We test our method with both fixed-priority and auction based protocols for coordinating the iterative plan ning process. Results are provided in collision avoidance simulations of feedback controlled plants.},
	booktitle = {Proceedings of the 2014 international conference on {Autonomous} agents and multi-agent systems ({AAMAS})},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Calliess, Jan-Peter and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2014},
	pages = {1109--1116},
	file = {2184/fp604-calliess.pdf}
}

@inproceedings{osborne_active_2010,
	title = {Active data selection for sensor networks with faults and changepoints},
	isbn = {1-4244-6695-4},
	abstract = {We describe a Bayesian formalism for the intelligent selection of observations from sensor networks that may intermittently undergo faults or changepoints. Such active data selection is performed with the goal of taking as few observations as necessary in order to maintain a reasonable level of uncertainty about the variables of interest. The presence of faults/changepoints is not always obvious and therefore our algorithm must first detect their occurrence. Having done so, our selection of observations must be appropriately altered. Faults corrupt our observations, reducing their impact; changepoints (abrupt changes in the characteristics of data) may require the transition to an entirely different sampling schedule. Our solution is to employ a Gaussian process formalism that allows for sequential time-series prediction about variables of interest along with a decision theoretic approach to the problem of selecting observations.},
	booktitle = {Advanced {Information} {Networking} and {Applications} ({AINA}), 2010 24th {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
	year = {2010},
	pages = {533--540},
	file = {1518/FaultySelection.pdf}
}

@inproceedings{mathibela_can_2012,
	title = {Can priors be trusted? learning to anticipate roadworks},
	isbn = {1-4673-3064-7},
	abstract = {This paper addresses the question of how much a previously obtained map of a road environment should be trusted for vehicle localisation, during autonomous driving, by assessing the probability that roadworks are being traversed. We compare two formulations of a roadwork prior: one based on Gaussian Process (GP) Classification and the other a more conventional Hidden Markov Model (HMM) in order to model correlations between nearby parts of a vehicle trajectory. Impor- tantly, our formulation allows this prior to be updated efficiently and repeatedly to gain an ever more accurate model of the environment over time. In the absence of, or in addition to, any in-situ observations, information from dedicated web resources can readily be incorporated into the framework. We evaluate our model using real data from an autonomous car and show that although the GP and HMM are roughly commensurate in terms of mapping roadworks, the GP provides a more powerful representation and lower prediction error. Our method allows us to truly map and anticipate roadworks on urban roads.},
	booktitle = {Intelligent {Transportation} {Systems} ({ITSC}), 2012 15th {International} {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Mathibela, Bonolo and Osborne, Michael A. and Posner, Ingmar and Newman, Paul},
	year = {2012},
	pages = {927--932},
	file = {2080/roadworks_2012.pdf}
}

@inproceedings{rogers_information_2008,
	title = {Information agents for pervasive sensor networks},
	isbn = {0-7695-3113-X},
	abstract = {In this paper, we describe an information agent, that resides on a mobile computer or personal digital assistant (PDA), that can autonomously acquire sensor readings from pervasive sensor networks (deciding when and which sensor to acquire readings from at any time). Moreover, it can perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental parameters will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and we describe how we use an iterative formulation of a multi-output Gaussian process to build a probabilistic model of the environmental parameters being measured by local sensors, and the correlations and delays that exist between them. We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
	booktitle = {Pervasive {Computing} and {Communications}, 2008. {PerCom} 2008. {Sixth} {Annual} {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Rogers, Alex and Ramchurn, Sarvapali D. and Jennings, Nicholas R. and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2008},
	pages = {294--299},
	file = {1644/rogers-InformationAgents.pdf}
}

@inproceedings{gunter_efficient_2014,
	title = {Efficient {Bayesian} {Nonparametric} {Modelling} of {Structured} {Point} {Processes}},
	url = {http://arxiv.org/abs/1407.6949},
	abstract = {This paper presents a Bayesian generative model for dependent Cox point processes, alongside an efficient inference scheme which scales as if the point processes were modelled independently. We can handle missing data naturally, infer latent structure, and cope with large numbers of observed processes. A further novel contribution enables the model to work effectively in higher dimensional spaces. Using this method, we achieve vastly improved predictive performance on both 2D and 1D real data, validating our structured approach.},
	booktitle = {30th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2014},
	file = {1447/1407.6949v1.pdf}
}

@inproceedings{lloyd_latent_2016,
	title = {Latent {Point} {Process} {Allocation}},
	url = {http://jmlr.org/proceedings/papers/v51/lloyd16.html},
	abstract = {We introduce a probabilistic model for the factorisation of continuous Poisson process rate functions. Our model can be thought of as a topic model for Poisson point processes in which each point is assigned to one of a set of latent rate functions that are shared across multiple outputs. We show that the model brings a means of incorporating structure in point process inference beyond the state-of-the-art. We derive an efficient variational inference scheme for the model based on sparse Gaussian processes that scales linearly in the number of data points. Finally, we demonstrate, using examples from spatial and temporal statistics, how the model can be used for discovering hidden structure with greater precision than standard frequentist approaches.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Lloyd, Chris and Gunter, Tom and Osborne, Michael A. and Roberts, Stephen J. and Nickson, Tom},
	year = {2016},
	pages = {389--397},
	file = {1064/Lloyd et al. - 2016 - Latent Point Process Allocation.pdf}
}

@inproceedings{garnett_active_2014,
	title = {Active {Learning} of {Linear} {Embeddings} for {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1310.6740},
	abstract = {We propose an active learning method for discovering low-dimensional structure in high-dimensional Gaussian process (GP) tasks. Such problems are increasingly frequent and important, but have hitherto presented severe practical difficulties. We further introduce a novel technique for approximately marginalizing GP hyperparameters, yielding marginal predictions robust to hyperparameter mis-specification. Our method offers an efficient means of performing GP regression, quadrature, or Bayesian optimization in high-dimensional spaces.},
	booktitle = {30th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Garnett, Roman and Osborne, Michael A. and Hennig, Philipp},
	year = {2014},
	note = {https://is.gd/orezZh},
	file = {1673/1310.6740v1.pdf}
}

@inproceedings{osborne_prediction_2012,
	title = {Prediction and {Fault} {Detection} of {Environmental} {Signals} with {Uncharacterised} {Faults}.},
	abstract = {Many signals of interest are corrupted by faults of an unknown type. We propose an approach that uses Gaussian processes and a general “fault bucket” to capture a priori uncharacterised faults, along with an approximate method for marginalising the potential faultiness of all observations. This gives rise to an efficient, flexible algorithm for the detection and automatic correction of faults. Our method is deployed in the domain of water monitoring and management, where it is able to solve several fault detection, correction, and prediction problems. The method works well despite the fact that the data is plagued with numerous difficulties, including missing observations, multiple discontinuities, nonlinearity and many unanticipated types of fault.},
	booktitle = {The {Association} for the {Advancement} of {Artificial} {Intelligence} {Conference} {On} {Artificial} {Intelligence} ({AAAI})},
	author = {Osborne, Michael A. and Garnett, Roman and Swersky, Kevin and De Freitas, Nando},
	year = {2012},
	note = {https://is.gd/9M0ZYE},
	file = {2015/Osborne_Garnett_Swersky_de_Freitas_fault_bucket_aaai_2012.pdf}
}

@inproceedings{gunter_sampling_2014,
	title = {Sampling for {Inference} in {Probabilistic} {Models} with {Fast} {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/1411.0439},
	abstract = {We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Gunter, Tom and Osborne, Michael A. and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J.},
	month = nov,
	year = {2014},
	note = {https://github.com/OxfordML/wsabi},
	keywords = {Statistics - Machine Learning},
	file = {1874/Gunter et al. - 2014 - Sampling for Inference in Probabilistic Models wit.pdf}
}

@inproceedings{osborne_gaussian_2009,
	title = {Gaussian processes for global optimization},
	abstract = {We introduce a novel Bayesian approach to global optimization using Gaussian processes. We frame the optimization of both noisy and noiseless functions as sequential decision problems, and introduce myopic and non-myopic solutions to them. Here our solutions can be tailored to exactly the degree of confidence we require of them. The use of Gaussian processes allows us to benefit from the incorporation of prior knowledge about our objective function, and also from any derivative observations. Using this latter fact, we introduce an innovative method to combat conditioning problems. Our algorithm demonstrates a significant improvement over its competitors in overall performance across a wide range of canonical test problems.},
	booktitle = {3rd international conference on learning and intelligent optimization ({LION}3)},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
	year = {2009},
	pages = {1--15},
	file = {1419/Osborne et al. - 2009 - Gaussian processes for global optimization.pdf}
}

@inproceedings{garnett_bayesian_2010,
	title = {Bayesian optimization for sensor set selection},
	isbn = {1-60558-988-8},
	doi = {10.1145/1791212.1791238},
	abstract = {We consider the problem of selecting an optimal set of sensors, as determined, for example, by the predictive accuracy of the resulting sensor network. Given an underlying metric between pairs of set elements, we introduce a natural metric between sets of sensors for this task. Using this metric, we can construct covariance functions over sets, and thereby perform Gaussian process inference over a function whose domain is a power set. If the function has additional inputs, our covariances can be readily extended to incorporate them—allowing us to consider, for example, functions over both sets and time. These functions can then be optimized using Gaussian process global optimization (GPGO). We use the root mean squared error (RMSE) of the predictions made using a set of sensors at a particular time as an example of such a function to be optimized; the optimal point specifies the best choice of sensor locations. We demonstrate the resulting method by dynamically selecting the best subset of a given set of weather sensors for the prediction of the air temperature across the United Kingdom.},
	booktitle = {Proceedings of the 9th {ACM}/{IEEE} {International} {Conference} on {Information} {Processing} in {Sensor} {Networks} ({IPSN})},
	publisher = {ACM},
	author = {Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2010},
	pages = {209--219},
	file = {1242/ipsn673-garnett.pdf}
}

@inproceedings{osborne_bayesian_2012,
	title = {Bayesian quadrature for ratios},
	abstract = {We describe a novel approach to quadrature for ratios of probabilistic integrals, such as are used to compute posterior probabilities. This approach offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the nonnegativity of our integrands, and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate. We demonstrate the efficacy of our method on data from the Kepler space telescope.},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J. and Hart, Christopher and Aigrain, Suzanne and Gibson, Neale},
	year = {2012},
	pages = {832--840},
	file = {1997/109_paper.pdf}
}

@inproceedings{fischer_recommending_2013,
	title = {Recommending energy tariffs and load shifting based on smart household usage profiling},
	isbn = {1-4503-1965-3},
	abstract = {We present a system and study of personalized energyrelated recommendation. AgentSwitch utilizes electricity usage data collected from users’ households over a period of time to realize a range of smart energy-related recommendations on energy tariffs, load detection and usage shifting. The web service is driven by a third party real-time energy tariff API (uSwitch), an energy data store, a set of algorithms for usage prediction, and appliance-level load disaggregation. We present the system design and user evaluation consisting of interviews and interface walkthroughs. We recruited participants from a previous study during which three months of their household’s energy use was recorded to evaluate personalized recommendations in AgentSwitch. Our contributions are a) a systems architecture for personalized energy services; and b) findings from the evaluation that reveal challenges in designing energy-related recommender systems. In response to the challenges we formulate design recommendations to mitigate barriers to switching tariffs, to incentivize load shifting, and to automate energy management.},
	booktitle = {Proceedings of the 2013 international conference on {Intelligent} user interfaces ({IUI})},
	publisher = {ACM},
	author = {Fischer, Joel E. and Ramchurn, Sarvapali D. and Osborne, Michael A. and Parson, Oliver and Huynh, Trung Dong and Alam, Muddasser and Pantidi, Nadia and Moran, Stuart and Bachour, Khaled and Reece, Steve},
	year = {2013},
	pages = {383--394},
	file = {1155/IUI2013-agentswitch-camera-ready.pdf}
}

@inproceedings{osborne_towards_2008,
	address = {Washington, DC, USA},
	series = {{IPSN} '08},
	title = {Towards {Real}-{Time} {Information} {Processing} of {Sensor} {Network} {Data} {Using} {Computationally} {Efficient} {Multi}-output {Gaussian} {Processes}},
	isbn = {978-0-7695-3157-1},
	url = {http://dx.doi.org/10.1109/IPSN.2008.25},
	doi = {10.1109/IPSN.2008.25},
	abstract = {In this paper, we describe a novel, computationally efficient algorithm that facilitates the autonomous acquisition of readings from sensor networks (deciding when and which sensor to acquire readings from at any time), and which can, with minimal domain knowledge, perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental variables will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and to this end, we describe a novel iterative formulation of a multi-output Gaussian process that can build and exploit a probabilistic model of the environmental variables being measured (including the correlations and delays that exist between them). We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
	urldate = {2014-08-30},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Information} {Processing} in {Sensor} {Networks} ({IPSN})},
	publisher = {IEEE Computer Society},
	author = {Osborne, Michael A. and Roberts, Stephen J. and Rogers, Alex and Ramchurn, Sarvapali D. and Jennings, Nicholas R.},
	year = {2008},
	keywords = {Gaussian processes, information processing, sensor network},
	pages = {109--120},
	file = {987/osborne-GaussianProcesses.pdf}
}

@inproceedings{garnett_sequential_2009,
	title = {Sequential {Bayesian} prediction in the presence of changepoints},
	isbn = {1-60558-516-5},
	abstract = {- ence of changepoints. Unlike previous ap- proaches, which focus on the problem of de- tecting and locating changepoints, our al- gorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary co- variance functions to be used in Gaussian process prediction that model such changes, then proceed to demonstrate how to effec- tively manage the hyperparameters associ- ated with those covariance functions. By us- ing Bayesian quadrature, we can integrate out the hyperparameters, allowing us to cal- culate the marginal predictive distribution. Furthermore, if desired, the posterior distri- bution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {ACM},
	author = {Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	pages = {345--352},
	file = {2076/changepoint.pdf}
}

@inproceedings{ramchurn_agentswitch_2013,
	title = {{AgentSwitch} : towards smart energy tariff selection},
	isbn = {1-4503-1993-9},
	abstract = {In this paper, we present AgentSwitch, a prototype agent-based platform to solve the electricity tariff selection problem. AgentSwitch incorporates novel algorithms to make predictions of hourly energy usage as well as detect (and suggest to the user) deferrable loads that could be shifted to off-peak times to maximise savings. To take advantage of group discounts from energy retailers, we develop a new scalable collective energy purchasing mechanism, based on the Shapley value, that ensures individual members of a collective (interacting through AgentSwitch) fairly share the discounts. To demonstrate the effectiveness of our algorithms we empirically evaluate them individually on real-world data (with up to 3000 homes in the UK) and show that they outperform the state of the art in their domains. Finally, to ensure individual components are accountable in providing recommendations, we provide a novel provenance-tracking service to record the flow of data in the system, and therefore provide users with a means of checking the provenance of suggestions from AgentSwitch and assess their reliability.},
	booktitle = {Proceedings of the 2013 international conference on {Autonomous} agents and multi-agent systems ({AAMAS})},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Ramchurn, Sarvapali D. and Osborne, Michael A. and Parson, Oliver and Rahwan, Talal and Maleki, Sasan and Reece, Steve and Huynh, Trung D. and Alam, Muddasser and Fischer, Joel E. and Rodden, Tom},
	year = {2013},
	pages = {981--988},
	file = {1977/AAMAS2013-agentswitch.pdf}
}

@inproceedings{briol_frank-wolfe_2015,
	title = {Frank-{Wolfe} {Bayesian} {Quadrature}: {Probabilistic} {Integration} with {Theoretical} {Guarantees}},
	shorttitle = {Frank-{Wolfe} {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/1506.02681},
	abstract = {There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A.},
	month = jun,
	year = {2015},
	keywords = {Statistics - Machine Learning},
	file = {1575/main.pdf}
}

@inproceedings{osborne_active_2012,
	title = {Active learning of model evidence using {Bayesian} quadrature},
	abstract = {Numerical integration is a key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a modelbased method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model’s hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Osborne, Michael A. and Duvenaud, David K. and Garnett, Roman and Rasmussen, Carl E. and Roberts, Stephen J. and Ghahramani, Zoubin},
	year = {2012},
	pages = {46--54},
	file = {1748/bbq_nips_final.pdf}
}

@inproceedings{sarkar_machine_2016,
	title = {A {Machine} {Learning} {Approach} to the {Prediction} of {Tidal} {Currents}.},
	url = {http://users.ox.ac.uk/~spet1235/ISOPE-2016-TPC-0969-final.pdf},
	abstract = {We propose the use of techniques from Machine Learning for the prediction of tidal currents. The classical methodology of harmonic analysis is widely used in the prediction of tidal currents and computer algorithms based on the method have been used for decades for the purpose. The approach determines parameters by minimizing the difference between the raw data and model output using the least squares optimization approach. However, although the approach is considered to be state-of-the-art, it possesses several drawbacks that can lead to significant prediction errors, especially at locations of fast tidal currents and ’noisy’ tidal signal. In general, careful selection of tidal constituents is required in order to achieve good predictions, and the underlying assumption of stationarity in time can restrict the applicability of the method to particular situations. There is a need for principled approaches which can handle uncertainty and accommodate noise in the data. In this work, we use Gaussian process, a Bayesian non-parametric technique, to predict tidal currents. The overall objective is to take advantage of the recent progress in machine learning to construct a robust yet efficient algorithm. The development can specifically benefit the tidal energy community, aiming to harness energy from location of fast tidal currents.},
	booktitle = {The {Proceedings} of {The} {Twenty}-sixth (2016) {International} {Ocean} {And} {Polar} {Engineering} {Conference}},
	author = {Sarkar, Dripta and Osborne, Michael and Adcock, Thomas},
	year = {2016},
	file = {923/Sarkar et al. - A Machine Learning Approach to the Prediction of T.pdf}
}

@inproceedings{gonzalez_glasses:_2016,
	title = {{GLASSES}: {Relieving} {The} {Myopia} {Of} {Bayesian} {Optimisation}},
	shorttitle = {{GLASSES}},
	url = {http://jmlr.org/proceedings/papers/v51/gonzalez16b.html},
	abstract = {We present GLASSES: Global optimisation with Look-Ahead through Stochastic
Simulation and Expected-loss Search. The majority of global optimisation
approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future.  This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Gonzalez, Javier and Osborne, Michael A. and Lawrence, Neil},
	year = {2016},
	note = {https://is.gd/P3KiBx},
	pages = {790--799},
	file = {1209/Gonzalez et al. - 2016 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf}
}

@inproceedings{cutajar_preconditioning_2016,
	title = {Preconditioning {Kernel} {Matrices}},
	url = {http://arxiv.org/abs/1602.06693},
	abstract = {The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget.},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Cutajar, Kurt and Osborne, Michael A. and Cunningham, John P. and Filippone, Maurizio},
	month = feb,
	year = {2016},
	note = {https://is.gd/365blF},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {1525/Cutajar et al. - 2016 - Preconditioning Kernel Matrices.pdf}
}

@inproceedings{lloyd_variational_2015,
	title = {Variational {Inference} for {Gaussian} {Process} {Modulated} {Poisson} {Processes}},
	url = {http://jmlr.org/proceedings/papers/v37/lloyd15.html},
	abstract = {We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.},
	booktitle = {Proceedings of {The} 32nd {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Lloyd, Chris and Gunter, Tom and Osborne, Michael A. and Roberts, Stephen},
	year = {2015},
	pages = {1814--1822},
	file = {1290/lloyd15.pdf}
}

@inproceedings{riar_energy_2016,
	title = {Energy management of a microgrid: {Compensating} for the difference between the real and predicted output power of photovoltaics},
	shorttitle = {Energy management of a microgrid},
	doi = {10.1109/PEDG.2016.7527042},
	abstract = {An increasing awareness of energy efficiency has led to the development of several improved converter topologies, semiconductor devices and control schemes for distributed energy resources, and, particularly, for microgrids. Recent advances in energy management systems (EMS) for microgrids have improved upon existing methods in several aspects, including prediction of power generated by photovoltaics (PV), and optimal management of electrical energy storage. However, the actual generated PV power may deviate from predictions for several reasons, such as rapid cloud changes or system faults. This paper contributes to the ongoing research on EMS control schemes by proposing a model predictive control (MPC) scheme that adapts to the difference between the actual and predicted output power of PV. The key benefit of this approach is its ability to rapidly adapt to varying operating conditions of the PV without increasing the computational burden of a typical MPC scheme. The feasibility of the scheme is demonstrated using simulations of 5 kW microgrid system compromising a 5 kW/400 Ah battery, 10 kW PV and 5 kW grid/load connection. The proposed scheme reduces variations in the state of charge (SOC) of a battery. The proposed scheme also reduces the energy taken from grid and this improvement in performance is a function of the difference between the actual and the predicted power.},
	booktitle = {2016 {IEEE} 7th {International} {Symposium} on {Power} {Electronics} for {Distributed} {Generation} {Systems} ({PEDG})},
	author = {Riar, Baljit and Lee, Jaehwa and Tosi, Alessandra and Duncan, Stephen and Osborne, Michael A. and Howey, David},
	month = jun,
	year = {2016},
	keywords = {Batteries, Battery storage system, EMS control schemes, Energy management, MPC scheme, Microgrids, Model predictive control, Optimisation, Optimization, PV, Power generation, Predictive control, State of charge, battery, battery state of charge variations reduction, computational burden, converter topology, distributed energy resource, distributed power generation, electrical energy storage optimal management, energy conservation, energy efficiency awareness, energy management system, energy management systems, energy storage, grid-load connection, microgrid, microgrid energy management system, model predictive control scheme, photovoltaic (PV) system, photovoltaic power systems, photovoltaics, power 10 kW, power 5 kW, power convertors, power generation control, power generation prediction, power grids, secondary cells, semiconductor device},
	pages = {1--7},
	file = {1340/Riar et al. - 2016 - Energy management of a microgrid Compensating for.pdf}
}

@inproceedings{rainforth_bayesian_2016,
	title = {Bayesian {Optimization} for {Probabilistic} {Programs}},
	url = {http://papers.nips.cc/paper/6421-bayesian-optimization-for-probabilistic-programs.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A and Wood, Frank},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {280--288},
	file = {2281/Rainforth et al. - 2016 - Bayesian Optimization for Probabilistic Programs.pdf}
}

@inproceedings{fruehwirt_bayesian_2017,
	title = {Bayesian {Gaussian} {Process} {Classification} from {Event}-{Related} {Brain} {Potentials} in {Alzheimer}’s {Disease}},
	volume = {10259},
	url = {https://is.gd/z5lLxE},
	abstract = {Event-related potentials (ERPs) have been shown to reflect neurodegenerative processes in Alzheimer’s disease (AD) and might qualify as non-invasive and cost-effective markers to facilitate the objectivization of AD assessment in daily clinical practice. Lately, the combination of multivariate pattern analysis (MVPA) and Gaussian process classification (GPC) has gained interest in the neuroscientific community. Here, we demonstrate how a MVPA-GPC approach can be applied to electrophysiological data. Furthermore, in order to account for the temporal information of ERPs, we develop a novel method that integrates interregional synchrony of ERP time signatures. By using real-life ERP recordings of a prospective AD cohort study (PRODEM), we empirically investigate the usefulness of the proposed framework to build neurophysiological markers for single subject classification tasks. GPC outperforms the probabilistic reference method in both tasks, with the highest AUC overall (0.802) being achieved using the new spatiotemporal method in the prediction of rapid cognitive decline.},
	booktitle = {Artificial {Intelligence} in {Medicine}: 16th {Conference} on {Artificial} {Intelligence} in {Medicine}, {AIME} 2017, {Vienna}, {Austria}, {June} 21-24, 2017, {Proceedings}},
	publisher = {Springer},
	author = {Fruehwirt, Wolfgang and Zhang, Pengfei and Gerstgrasser, Matthias and Grossegger, Dieter and Schmidt, Reinhold and Benke, Thomas and Dal-Bianco, Peter and Ransmayr, Gerhard and Weydemann, Leonard and Garn, Heinrich and Waser, Markus and Osborne, Michael A and Dorffner, Georg},
	year = {2017},
	pages = {65},
	file = {2312/Bayesian_Gaussian_Process_Classification_from_Event-Related_Brain_Potentials_in_AD.pdf}
}

@inproceedings{fitzsimons_bayesian_2017,
	title = {Bayesian {Inference} of {Log} {Determinants}},
	url = {https://arxiv.org/abs/1704.01445},
	abstract = {The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.},
	urldate = {2017-06-21},
	booktitle = {Uncertainty in {Artificial} {Intelligence} (to appear)},
	author = {Fitzsimons, Jack and Cutajar, Kurt and Osborne, Michael and Roberts, Stephen and Filippone, Maurizio},
	year = {2017},
	file = {2307/Fitzsimons et al. - 2017 - Bayesian Inference of Log Determinants.pdf}
}

@inproceedings{bewsher_distribution_2017,
	title = {Distribution of {Gaussian} {Process} {Arc} {Lengths}},
	url = {http://proceedings.mlr.press/v54/bewsher17a.html},
	abstract = {We present the first treatment of the arc length of the GP with more than a single output dimension. GPs are commonly used for tasks such as trajectory modelling, where path length is a crucial ...},
	language = {en},
	urldate = {2017-06-21},
	booktitle = {{PMLR}},
	author = {Bewsher, Justin and Tosi, Alessandra and Osborne, Michael and Roberts, Stephen},
	month = apr,
	year = {2017},
	pages = {1412--1420},
	file = {2310/Bewsher et al. - 2017 - Distribution of Gaussian Process Arc Lengths.pdf}
}
@article{paul_alternating_2016,
	title = {Alternating {Optimisation} and {Quadrature} for {Robust} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.07496},
	abstract = {Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables - state features that are randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding an optimal policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but have a considerable impact on determining the optimal policy. We provide experimental results demonstrating our approach learning more efficiently than existing methods.},
	urldate = {2016-08-26},
	journal = {arXiv preprint arXiv:1605.07496},
	author = {Paul, Supratik and Ciosek, Kamil and Osborne, Michael A. and Whiteson, Shimon},
	year = {2016},
	keywords = {preprint},
	file = {1388/Paul et al. - 2016 - Alternating Optimisation and Quadrature for Robust.pdf}
}

@article{nickson_blitzkriging_2015,
	title = {Blitzkriging : {Kronecker}-structured {Stochastic} {Gaussian} {Processes}},
	shorttitle = {Blitzkriging},
	url = {http://arxiv.org/abs/1510.07965},
	abstract = {We present Blitzkriging, a new approach to fast inference for Gaussian processes, applicable to regression, optimisation and classification. State-of-the-art (stochastic) inference for Gaussian processes on very large datasets scales cubically in the number of 'inducing inputs', variables introduced to factorise the model. Blitzkriging shares state-of-the-art scaling with data, but reduces the scaling in the number of inducing points to approximately linear. Further, in contrast to other methods, Blitzkriging: does not force the data to conform to any particular structure (including grid-like); reduces reliance on error-prone optimisation of inducing point locations; and is able to learn rich (covariance) structure from the data. We demonstrate the benefits of our approach on real data in regression, time-series prediction and signal-interpolation experiments.},
	journal = {arXiv:1510.07965 [stat]},
	author = {Nickson, Thomas and Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen},
	month = oct,
	year = {2015},
	keywords = {preprint},
	file = {1299/Nickson et al. - 2015 - Blitzkriging Kronecker-structured Stochastic Gaus.pdf}
}

@techreport{reece_anomaly_2009,
	title = {Anomaly detection and removal using nonstationary {Gaussian} processes},
	abstract = {This paper proposes a novel Gaussian process approach to
fault removal in time-series data. Fault removal does not delete
the faulty signal data but, instead, massages the fault from
the data. We assume that only one fault occurs at any one
time and model the signal by two separate non-parametric
Gaussian process models for both the physical phenomenon
and the fault. In order to facilitate fault removal we introduce
the Markov Region Link kernel for handling non-stationary
Gaussian Processes. This kernel is piece-wise stationary but
guarantees that functions generated by it and their derivatives
(when required) are everywhere continuous. We apply this
kernel to the removal of drift and bias errors in faulty sensor
data and also to the recovery of EOG artifact corrupted EEG
signals.},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Reece, Steve and Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	keywords = {preprint},
	file = {1108/reece2009.pdf}
}

@article{gillani_communication_2014,
	title = {Communication {Communities} in {MOOCs}},
	abstract = {Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.},
	journal = {arXiv preprint arXiv:1403.4640},
	author = {Gillani, Nabeel and Eynon, Rebecca and Osborne, Michael A. and Hjorth, Isis and Roberts, Stephen},
	year = {2014},
	keywords = {preprint},
	file = {1015/1403.4640.pdf}
}

@article{briol_probabilistic_2015,
	title = {Probabilistic {Integration}: {A} {Role} for {Statisticians} in {Numerical} {Analysis}?},
	url = {http://arxiv.org/abs/1512.00933},
	abstract = {Probabilistic numerical methods aim to model numerical error as a source of epistemic uncertainty that is subject to probabilistic analysis and reasoning, enabling the principled propagation of numerical uncertainty through a computational pipeline. In this paper we focus on numerical methods for integration. We present probabilistic (Bayesian) versions of both Markov chain and Quasi Monte Carlo methods for integration and provide rigorous theoretical guarantees for convergence rates, in both posterior mean and posterior contraction. The performance of probabilistic integrators is guaranteed to be no worse than non-probabilistic integrators and is, in many cases, asymptotically superior. These probabilistic integrators therefore enjoy the "best of both worlds", leveraging the sampling efficiency of advanced Monte Carlo methods whilst being equipped with valid probabilistic models for uncertainty quantification. Several applications and illustrations are provided, including examples from computer vision and system modelling using non-linear differential equations. A survey of open challenges in probabilistic integration is provided.},
	journal = {arXiv:1512.00933 [cs, math, stat]},
	author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning, preprint},
	file = {1412/Briol et al. - 2015 - Probabilistic Integration.pdf}
}

@misc{osborne_epistemic_2008,
	title = {Epistemic {Uncertainty} in {Quantum} {Mechanics}},
	author = {Osborne, Michael A.},
	month = apr,
	year = {2008},
	keywords = {preprint},
	file = {1460/Osborne2008Epistemic_uncertainty_in_quantum_mechanics.pdf}
}

@article{fitzsimons_improved_2016,
	title = {Improved stochastic trace estimation using mutually unbiased bases},
	url = {http://arxiv.org/abs/1608.00117},
	abstract = {We examine the problem of estimating the trace of a matrix A when given access to an oracle which computes x† A x for an input vector x. We make use of the basis vectors from a set of mutually unbiased bases, widely studied in the field of quantum information processing, in the selection of probing vectors x. This approach offers a new state of the art single shot sampling variance while requiring only O(log(n)) random bits to generate each vector. This significantly improves on traditional methods such as Hutchinson's and Gaussian estimators in terms of the number of random bits required and worst case sample variance.},
	journal = {arXiv:1608.00117 [quant-ph]},
	author = {Fitzsimons, Jack K. and Osborne, Michael A. and Roberts, Stephen J. and Fitzsimons, Joe F.},
	month = jul,
	year = {2016},
	keywords = {preprint},
	file = {1635/Fitzsimons et al. - 2016 - Improved stochastic trace estimation using mutuall.pdf}
}

@article{hutter_kernel_2013,
	title = {A {Kernel} for {Hierarchical} {Parameter} {Spaces}},
	url = {http://arxiv.org/abs/1310.5738},
	abstract = {We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite.},
	journal = {arXiv preprint arXiv:1310.5738},
	author = {Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {2152/1310.5738v1.pdf}
}

@inproceedings{swersky_raiders_2013,
	title = {Raiders of the lost architecture\_ {Kernels} for {Bayesian} optimization in conditional parameter spaces},
	abstract = {In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels.},
	booktitle = {{NIPS} workshop on {Bayesian} {Optimization} in theory and practice ({BayesOpt}’13)},
	author = {Swersky, Kevin and Duvenaud, David and Snoek, Jasper and Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {1173/hier-kern-workshop.pdf}
}

@techreport{osborne_gaussian_2007,
	title = {Gaussian processes for prediction},
	abstract = {We propose a powerful prediction algorithm built upon Gaussian processes (GPs). They are
particularly useful for their flexibility, facilitating accurate prediction even in the absence of strong
physical models.
GPs further allow us to work within a complete Bayesian probabilistic framework. As such, we
show how the hyperparameters of our system can be marginalised by use of Bayesian Monte Carlo, a
principled method of approximate integration. We employ the error bars of our GP’s predictions as
a means to select only the most informative data to store. This allows us to introduce an iterative
formulation of the GP to give a dynamic, on-line algorithm. We also show how our error bars can be
used to perform active data selection, allowing the GP to select where and when it should next take a
measurement.
We demonstrate how our methods can be applied to multi-sensor prediction problems where data
may be missing, delayed and/or correlated. In particular, we present a real network of weather sensors
as a testbed for our algorithm.},
	number = {PARG-07-01},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Osborne, Michael A. and Roberts, Stephen J.},
	year = {2007},
	keywords = {preprint},
	file = {1133/PARG-07-01.pdf}
}

@article{nickson_automated_2014,
	title = {Automated {Machine} {Learning} on {Big} {Data} using {Stochastic} {Algorithm} {Tuning}},
	url = {http://arxiv.org/abs/1407.7969},
	abstract = {We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.},
	journal = {arXiv preprint arXiv:1407.7969},
	author = {Nickson, Thomas and Osborne, Michael A. and Reece, Steven and Roberts, Stephen J.},
	year = {2014},
	note = {https://is.gd/e3JAVg},
	keywords = {preprint},
	file = {1051/1407.7969v1.pdf}
}

@article{salas_variational_2015,
	title = {A {Variational} {Bayesian} {State}-{Space} {Approach} to {Online} {Passive}-{Aggressive} {Regression}},
	url = {http://arxiv.org/abs/1509.02438},
	abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
	journal = {arXiv:1509.02438 [stat]},
	author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
	month = sep,
	year = {2015},
	keywords = {preprint},
	file = {1838/Salas et al. - 2015 - A Variational Bayesian State-Space Approach to Onl.pdf}
}

@article{rizvi_novel_2017,
	title = {A {Novel} {Approach} to {Forecasting} {Financial} {Volatility} with {Gaussian} {Process} {Envelopes}},
	url = {https://arxiv.org/abs/1705.00891},
	abstract = {In this paper we use Gaussian Process (GP) regression to propose a novel approach for predicting volatility of financial returns by forecasting the envelopes of the time series. We provide a direct comparison of their performance to traditional approaches such as GARCH. We compare the forecasting power of three approaches: GP regression on the absolute and squared returns; regression on the envelope of the returns and the absolute returns; and regression on the envelope of the negative and positive returns separately. We use a maximum a posteriori estimate with a Gaussian prior to determine our hyperparameters. We also test the effect of hyperparameter updating at each forecasting step. We use our approaches to forecast out-of-sample volatility of four currency pairs over a 2 year period, at half-hourly intervals. From three kernels, we select the kernel giving the best performance for our data. We use two published accuracy measures and four statistical loss functions to evaluate the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform 20\% better than a random walk model, and 50\% better than GARCH for the same data.},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1705.00891},
	author = {Rizvi, Syed Ali Asad and Roberts, Stephen J. and Osborne, Michael A. and Nyikosa, Favour},
	year = {2017},
	keywords = {preprint},
	file = {2301/Rizvi et al. - 2017 - A Novel Approach to Forecasting Financial Volatili.pdf}
}

@article{fitzsimons_entropic_2017,
	title = {Entropic {Trace} {Estimates} for {Log} {Determinants}},
	url = {https://arxiv.org/abs/1704.07223},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1704.07223},
	author = {Fitzsimons, Jack and Granziol, Diego and Cutajar, Kurt and Osborne, Michael and Filippone, Maurizio and Roberts, Stephen},
	year = {2017},
	keywords = {preprint},
	file = {2304/Fitzsimons et al. - 2017 - Entropic Trace Estimates for Log Determinants.pdf}
}

@article{mcleod_practical_2017,
	title = {Practical {Bayesian} {Optimization} for {Variable} {Cost} {Objectives}},
	url = {http://arxiv.org/abs/1703.04335},
	abstract = {We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems.},
	urldate = {2017-06-30},
	journal = {arXiv:1703.04335 [stat]},
	author = {McLeod, Mark and Osborne, Michael A. and Roberts, Stephen J.},
	month = mar,
	year = {2017},
	keywords = {preprint},
	file = {2401/McLeod et al. - 2017 - Practical Bayesian Optimization for Variable Cost .pdf}
}

@article{rontsis_distributionally_2017,
	title = {Distributionally {Robust} {Optimization} {Techniques} in {Batch} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1707.04191},
	abstract = {We propose a novel, theoretically-grounded, acquisition function for batch Bayesian optimisation informed by insights from distributionally robust optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function - which requires a multi-dimensional Gaussian Expectation over a piecewise affine function - and is computed by evaluating instead the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. We show that, unlike alternative approaches including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be calculated exactly as the solution of a convex optimization problem in the form of a tractable semidefinite program (SDP). Moreover, we prove that the solution of this SDP also yields exact numerical derivatives, which enable efficient optimisation of the acquisition function. Numerical results suggest that our acquisition function performs very similar to the computationally intractable exact Expected Improvement and considerably better than other heuristics.},
	urldate = {2017-07-14},
	journal = {arXiv:1707.04191 [stat]},
	author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
	month = jul,
	year = {2017},
	note = {https://github.com/oxfordcontrol/Bayesian-Optimization},
	keywords = {preprint},
	file = {2460/Rontsis et al. - 2017 - Distributionally Robust Optimization Techniques in.pdf}
}

@article{rainforth_bayesian_2017,
	title = {Bayesian {Optimization} for {Probabilistic} {Programs}},
	url = {http://arxiv.org/abs/1707.04314},
	abstract = {We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.},
	urldate = {2017-07-17},
	journal = {arXiv:1707.04314 [cs, stat]},
	author = {Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A. and Wood, Frank},
	month = jul,
	year = {2017},
	keywords = {preprint},
	file = {2464/Rainforth et al. - 2017 - Bayesian Optimization for Probabilistic Programs.pdf}
}
@techreport{frey_technology_2015,
  type = {Citi GPS Report},
  title = {Technology at {{Work}}: {{The Future}} of {{Innovation}} and {{Employment}}},
  shorttitle = {Technology at {{Work}}},
  abstract = {Technology at Work: The Future of Innovation and Employment, is the latest Citi GPS report from the Oxford Martin School and Citi. It explores trends in automation and points to sluggish job creation caused partly by increasing automation, and argues that secular stagnation in the digital age can only be avoided by a shift towards inclusive growth.

The authors highlight the key challenges, explore some of the new technology brought on by the digital age and set out an agenda for change, calling for long term thinking to mitigate the negative effects of an ever more automated and digital economy.

Technology at Work marks the start of a new programme of research supported by Citi, the Oxford Martin Programme on Technology and Employment.},
  timestamp = {2017-06-20T09:15:48Z},
  author = {Frey, Carl Benedikt and Osborne, Michael A},
  year = {2015},
  file = {1480/GPS_final.pdf}
}

@techreport{frey_agiletown_2014,
  title = {Agiletown : The Relentless March of Technology and {{London}}'s Response},
  abstract = {Our London Futures insights series focuses on the London economy and what it needs to do to maintain and reinforce its position as a leading global business hub. Our latest report in the programme, Agiletown: the relentless march of technology and London's response, focuses on the challenges and opportunities that technology presents to London.

The findings indicate that a significant shift is occurring in the labour market. Jobs that do not need to be done in London, or can be fully replaced by technology, will continue to leave the city. However, the job losses will be outweighed by new jobs requiring skills that involve creativity, complex problem-solving and high technical content.

The report brings together research from Oxford Martin School academics Carl Benedikt Frey and Michael A Osborne on the potential impact of automation on jobs in the UK and London over the next two decades, and a Deloitte survey of 100 London based organisations, exploring the new jobs that will be created, the skills that will be needed, and the implications for current working practices.},
  timestamp = {2017-06-29T00:11:29Z},
  institution = {Deloitte},
  author = {Frey, Carl Benedikt and Osborne, Michael A.},
  month = oct,
  year = {2014},
  file = {1892/uk-london-futures-agiletown-nov-14.pdf}
}

@techreport{bakhshi_creativity_2015,
  title = {Creativity {{Vs Robots}}},
  abstract = {This report explores future automation and creativity in the UK and US workforces. We find that creative jobs will be much more resistant to automation than most other jobs.},
  timestamp = {2017-06-20T08:45:35Z},
  urldate = {2015-04-24},
  institution = {Nesta},
  author = {Bakhshi, Hasan and Frey, Carl Benedikt and Osborne, Michael A.},
  year = {2015},
  file = {1866/creativity_vs._robots_wv.pdf}
}

@techreport{frey_technology_2016,
  type = {Citi GPS Report},
  title = {Technology at {{Work}} v2: {{The Future Is Not What It Used}} to {{Be}}},
  shorttitle = {Technology at {{Work}} v2.0},
  abstract = {Technology at Work v2.0: The Future Is Not What It Used to Be, produced by the Oxford Martin School and Citi, provides in-depth analysis of the vulnerabilities of countries and cities to job automation, explores what automation will mean for traditional models of economic growth, and considers how governments can prepare for the potentially disruptive impacts of job automation on society.

It builds on 2013 research by Carl Benedikt Frey and Michael Osborne which found that 47 per cent of US jobs were at risk of automation over the next two decades, and on the first Technology at Work Citi GPS report, published in 2015. 

As well as collaborating on the Citi GPS series of reports, the School has partnered with Citi to create a new programme of research, the Oxford Martin Programme on Technology and Employment, to investigate the implications of a rapidly changing technological landscape for economies and societies.},
  timestamp = {2017-06-20T09:15:36Z},
  author = {Frey, Carl Benedikt and Osborne, Michael A.},
  year = {2016},
  file = {1204/Citi_GPS_Technology_Work_2.pdf}
}



@inproceedings{calliess_towards_2012,
	title = {Towards auction-based multi-agent collision-avoidance under continuous stochastic dynamics},
	abstract = {We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state tra jectories with adjustably high certainty, while aiming for low social cost. To this end we describe a collision-detection module based on a distribution-independent probabilistic bound. We employ an optimization-based approach to incrementally alter plans until all collisions are avoided with sufficiently high confidence. We consider the case of feedback controlled agents, and alter plans by introducing new setpoints to the agents’ controllers. In the context of a simple stochastic path planning scenario, we compare two alternative strategies for finding such setpoints.
Due to their practical importance, multi-agent collision avoidance and control have been extensively studied across different communities including AI, robotics and control. However, these works typically assume linear and discrete dynamic models; by contrast, our work intends to overcome these limitations and to present solutions for continuous state space. While our current experiments were conducted with linear stochastic differential equation (SDE) models with state-independent noise (yielding Gaussian processes) our method is also applicable to non-Gaussian cases with state-dependent uncertainties.
To ensure collision avoidance yields low social cost across the entire agent collective, we compare two different coordination mechanisms. Firstly, we consider a simple fixed-priority scheme Erdmann \& Lozano-Perez (1987), and secondly, we modify an auction-based coordination protocol Calliess et al. (2011) to work in our continuous setting. In contrast to pre-existing work in auction-style multi-agent planning (e.g. Tovey et al. (2005); Lagoudakis et al. (2005); Calliess et al. (2011)) and multi-agent collision avoidance (e.g. Kostic et al. (2010); Ayanian \& Kumar (2010)), we avoid a priori discretizations of space and time and present solutions for continuous time and state space.},
	booktitle = {{ICML}-2012, {Workshop} on {Markets}, {Mechanisms}, and {Multi}-{Agent} {Models}-{Examining} the {Interaction} of {Machine} {Learning} and {Economics}},
	author = {Calliess, Jan-P. and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2012},
	keywords = {workshop},
	file = {1668/calliess_ICML_workshop_2012.pdf}
}

@inproceedings{calliess_towards_2012-1,
	title = {Towards optimization-based multi-agent collision-avoidance under continuous stochastic dynamics},
	abstract = {In our ongoing work, we aim to control a team of agents so as to achieve a prescribed goal state while being confident that collisions with other agents are avoided. Each agent is associated with a feedback controlled plant, whose continuous state trajectories follow some stochastic differential dynamics. To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and employ a fixed priority method to resolve collisions. Due to their practical importance, multi-agent collision avoidance and control have been extensively studied across different communities including AI, robotics and control. However, these works typically assume linear and discrete dynamic models; by contrast, our work intends to overcome these limitations and to present solutions for continuous state space. While our current experiments were conducted with linear stochastic differential equation (SDE) models with state-independent noise (yielding Gaussian processes) we believe that our approach could also be applicable to nonGaussian cases with state-dependent uncertainties.},
	booktitle = {Workshops at the {Twenty}-{Sixth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Calliess, Jan-Peter and Osborne, Michael A. and Roberts, Stephen},
	year = {2012},
	keywords = {workshop},
	file = {1935/TowardsOptimization.pdf}
}

@inproceedings{osborne_machine_2011,
	title = {A machine learning approach to pattern detection and prediction for environmental monitoring and water sustainability},
	abstract = {We describe one of the successful products of a research partnership among several academic institutions (CMU, Oxford and UBC) and a water monitoring company (Aquatic Informatics). Water monitoring sensors are very diverse and remotely distributed. They produce vast quantities of data. The data itself is nonlinear and nonstationary. In addition, unanticipated environmental conditions and limitations in the sensing and communications hardware cause the data to be corrupted by previously uncharacterized nonlinearities, missing observations, spikes and multiple discontinuities. To improve the quality of the data and the monitoring process, this paper introduces an approach that uses Gaussian processes and a general “fault bucket” to capture a priori uncharacterized faults, along with an approximate method for marginalizing the potential faultiness of all observations. This gives rise to an efficient, flexible algorithm for the detection and automatic correction of faults. The probabilistic nature of the method is ideal for reporting uncertainty estimates to human operators. The approach can also be applied to detect patterns, other than faults, which are of great environmental significance. We present a fish sustainability example, where specific patterns in water level need to be detected so that fish don’t get trapped and die in shallow pools.},
	booktitle = {Proc. {Workshop} on {Machine} {Learning} for {Global} {Challenges}},
	author = {Osborne, Michael A. and Garnett, Roman and Swersky, Kevin and de Freitas, Nando},
	year = {2011},
	note = {https://is.gd/9M0ZYE},
	keywords = {workshop},
	file = {1628/osborne_et_al_mlgc_2011.pdf}
}

@inproceedings{rainforth_bayesian_2015,
	title = {Bayesian {Optimization} for {Probabilistic} {Programs}},
	url = {http://www.robots.ox.ac.uk/~twgr/assets/pdf/rainforth2015BOPP.pdf},
	abstract = {We outline a general purpose framework for black-box marginal maximum a pos- teriori estimation of probabilistic program variables using Bayesian optimization with Gaussian processes.  We introduce the concept of an optimization query, whereby a probabilistic program returns an infinite lazy sequence of increasingly optimal estimates, and explain how a general purpose program transformation would allow the evidence of any probabilistic program, and therefore any graphical model, to be optimized with respect to an arbitrary subset of its variables.},
	booktitle = {Workshop on "{Black} {Box} {Learning} and {Inference}" at {NIPS} 2015},
	author = {Rainforth, Tom and van de Meent, Jan-Willem and Osborne, Michael A. and Wood, Frank},
	year = {2015},
	keywords = {workshop},
	file = {1088/rainforth2015BOPP.pdf}
}

@inproceedings{gibson_nir_2011,
	title = {{NIR} {Transmission} {Spectra} of {HD}189733: {Application} of {Gaussian} {Processes} for {Removing} {Systematics}},
	volume = {2},
	url = {http://goo.gl/4DmFW4},
	abstract = {The interpretation of HST transmission spectroscopy signals has recently
been the subject of much debate, in particular the NIR NICMOS data of HD
189733. At optical wavelengths, a high-altitude haze has been confirmed
with both STIS and ACS, whereas the presence of molecules has been
claimed with NICMOS. However, this detection of molecules has been
disputed based on the ad hoc model used to remove the systematics, the
choice of which changes the interpretation of the transmission signal.
Here, we introduce a powerful new technique, Gaussian Processes (GPs),
to model the systematics and simultaneously extract the transmission
spectrum, and demonstrate its application to the NICMOS data. GPs are a
Bayesian technique widely used in the machine learning community, which
allow us to define a distribution over functions. Rather than impose a
strict, functional form of systematics correction, we marginalise over
potentially infinite numbers of basis functions, effectively inferring
the form of the systematics correction from the data itself. This
results in a more robust interpretation of the signal. We also present
similar analyses of HST/WFC3 observations of HD 189733, which bridge the
gap between the current optical and NIR spectrum.},
	booktitle = {{AAS}/{Division} for {Extreme} {Solar} {Systems} {Abstracts}},
	author = {Gibson, Neale and Aigrain, Suzanne and Roberts, Stephen J. and Evans, Tom and Osborne, Michael A. and Pont, Frederic and Sing, David K.},
	year = {2011},
	keywords = {workshop},
	pages = {1106}
}

@inproceedings{nyikosa_adaptive_2015,
	title = {Adaptive {Bayesian} {Optimisation} for {Online} {Portfolio} {Selection}},
	abstract = {We present a Bayesian approach for online portfolio selection, a fundamental problem in computational finance. We pose the problem as the global optimi- sation of an expensive, time-varying, black-box function. As the optimum is itself dynamic, we use a model that allows us to capture time-dependent patterns of the function and to provide sequential decision processes that enable us to select optimal portfolios to invest in an online manner.},
	booktitle = {Workshop on {Bayesian} {Optimization} at {NIPS} 2015},
	author = {Nyikosa, Favour and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2015},
	keywords = {workshop},
	file = {1566/NyikosaOsborneRobertsNipsBayesopt2015.pdf}
}

@inproceedings{aigrain_gaussian_2011,
	title = {Gaussian {Processes} : the {Next} {Step} in {Exoplanet} {Data} {Analysis}},
	volume = {2},
	abstract = {When searching for or characterising exoplanets, we typically need to isolate a deterministic signal from stochastic processes-astrophysical or instrumental" noise"-in time-series data. Gaussian processes (GPs) enable us to construct distributions over random functions, and to infer the properties of" signal" and" noise" in a way that is both flexible and robust.},
	booktitle = {{AAS}/{Division} for {Extreme} {Solar} {Systems} {Abstracts}},
	author = {Aigrain, Suzanne and Gibson, Neale P. and Roberts, Stephen J. and Evans, Tom and McQuillan, Amy and Reece, Steve and Osborne, Michael A.},
	year = {2011},
	keywords = {workshop},
	pages = {1105},
	file = {2053/algrain_suzanne.pdf}
}
@phdthesis{osborne_bayesian_2010,
	title = {Bayesian {Gaussian} {Processes} for {Sequential} {Prediction}, {Optimisation} and {Quadrature}},
	abstract = {We develop a family of Bayesian algorithms built around Gaussian processes for various problems posed by sensor networks. We firstly introduce an iterative Gaussian process for multi-sensor inference problems, and show how our algorithm is able to cope with data that may be noisy, missing, delayed and/or correlated. Our algorithm can also effectively manage data that features changepoints, such as sensor faults. Extensions to our algorithm allow us to tackle some of the decision problems faced in sensor networks, including observation scheduling. Along these lines, we also propose a general method of global optimisation, Gaussian process global optimisation (GPGO), and demonstrate how it may be used for sensor placement.

Our algorithms operate within a complete Bayesian probabilistic framework. As such, we show how the hyperparameters of our system can be marginalised by use of Bayesian quadrature, a principled method of approximate integration. Similar tech niques also allow us to produce full posterior distributions for any hyperparameters of interest, such as the location of changepoints. We frame the selection of the positions of the hyperparameter samples required by Bayesian quadrature as a decision prob lem, with the aim of minimising the uncertainty we possess about the values of the integrals we are approximating. Taking this approach, we have developed sampling for Bayesian quadrature (SBQ), a principled competitor to Monte Carlo methods.

We conclude by testing our proposals on real weather sensor networks. We further benchmark GPGO on a wide range of canonical test problems, over which it achieves a significant improvement on its competitors. Finally, the efficacy of SBQ is demonstrated in the context of both prediction and optimisation.},
	school = {PhD thesis, University of Oxford},
	author = {Osborne, Michael},
	year = {2010},
	file = {2160/full_thesis.pdf}
}