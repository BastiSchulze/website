@inproceedings{cohen2020pessimism,
    title = {Pessimism About Unknown Unknowns Inspires Conservatism},
    url = {https://pdfs.semanticscholar.org/cece/bc0c325a9fc58e78d82a42c8b3f2d9bce769.pdf},
    abstract = {If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ``unprecedented events'' with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.},
    language = {en},
    booktitle = {Conference on Learning Theory},
    author = {Cohen, Michael K and Hutter, Marcus},
    year = {2020},
    pages = {1--30},
    file = {cohen2020pessimism.pdf}
}


@inproceedings{nguyen2020knowing,
    title = {Knowing The What But Not The Where in Bayesian Optimization},
    url = {},
    abstract = {Bayesian optimization has demonstrated impressive success in finding the optimum input x and
output f* = f(x*) = max f(x) of a black-box function f. In some applications, however, the optimum output f*
is known in advance and the goal is to find the corresponding optimum input
x*. In this paper, we consider a new setting in BO in which the knowledge of the optimum output f is available. Our goal is to exploit the
knowledge about f* to search for the input x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information
about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization. We show that our approaches work intuitively and give quantitatively better performance against
standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.},
    language = {en},
    urldate = {},
    booktitle = {International Conference on Machine Learning},
    author = {Nguyen, Vu and Osborne, Michael},
    year = {2020},
    note = {Link to github code, if there is code},
    pages = {},
    file = {nguyen2020knowing.pdf}
}
