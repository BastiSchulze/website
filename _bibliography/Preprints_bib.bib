
@article{paul_alternating_2016,
	title = {Alternating {Optimisation} and {Quadrature} for {Robust} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.07496},
	urldate = {2016-08-26},
	journal = {arXiv preprint arXiv:1605.07496},
	author = {Paul, Supratik and Ciosek, Kamil and Osborne, Michael A. and Whiteson, Shimon},
	year = {2016},
	note = {00003},
	keywords = {preprint},
	file = {1388/Paul et al. - 2016 - Alternating Optimisation and Quadrature for Robust.pdf}
}

@article{nickson_blitzkriging_2015,
	title = {Blitzkriging : {Kronecker}-structured {Stochastic} {Gaussian} {Processes}},
	shorttitle = {Blitzkriging},
	url = {http://arxiv.org/abs/1510.07965},
	abstract = {We present Blitzkriging, a new approach to fast inference for Gaussian processes, applicable to regression, optimisation and classification. State-of-the-art (stochastic) inference for Gaussian processes on very large datasets scales cubically in the number of 'inducing inputs', variables introduced to factorise the model. Blitzkriging shares state-of-the-art scaling with data, but reduces the scaling in the number of inducing points to approximately linear. Further, in contrast to other methods, Blitzkriging: does not force the data to conform to any particular structure (including grid-like); reduces reliance on error-prone optimisation of inducing point locations; and is able to learn rich (covariance) structure from the data. We demonstrate the benefits of our approach on real data in regression, time-series prediction and signal-interpolation experiments.},
	journal = {arXiv:1510.07965 [stat]},
	author = {Nickson, Thomas and Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen},
	month = oct,
	year = {2015},
	keywords = {preprint},
	file = {1299/Nickson et al. - 2015 - Blitzkriging Kronecker-structured Stochastic Gaus.pdf}
}

@techreport{reece_anomaly_2009,
	title = {Anomaly detection and removal using nonstationary {Gaussian} processes},
	abstract = {This paper proposes a novel Gaussian process approach to
fault removal in time-series data. Fault removal does not delete
the faulty signal data but, instead, massages the fault from
the data. We assume that only one fault occurs at any one
time and model the signal by two separate non-parametric
Gaussian process models for both the physical phenomenon
and the fault. In order to facilitate fault removal we introduce
the Markov Region Link kernel for handling non-stationary
Gaussian Processes. This kernel is piece-wise stationary but
guarantees that functions generated by it and their derivatives
(when required) are everywhere continuous. We apply this
kernel to the removal of drift and bias errors in faulty sensor
data and also to the recovery of EOG artifact corrupted EEG
signals.},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Reece, Steve and Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	keywords = {preprint},
	file = {1108/reece2009.pdf}
}

@article{gillani_communication_2014,
	title = {Communication {Communities} in {MOOCs}},
	abstract = {Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.},
	journal = {arXiv preprint arXiv:1403.4640},
	author = {Gillani, Nabeel and Eynon, Rebecca and Osborne, Michael A. and Hjorth, Isis and Roberts, Stephen},
	year = {2014},
	keywords = {preprint},
	file = {1015/1403.4640.pdf}
}

@article{briol_probabilistic_2015,
	title = {Probabilistic {Integration}: {A} {Role} for {Statisticians} in {Numerical} {Analysis}?},
	url = {http://arxiv.org/abs/1512.00933},
	abstract = {Probabilistic numerical methods aim to model numerical error as a source of epistemic uncertainty that is subject to probabilistic analysis and reasoning, enabling the principled propagation of numerical uncertainty through a computational pipeline. In this paper we focus on numerical methods for integration. We present probabilistic (Bayesian) versions of both Markov chain and Quasi Monte Carlo methods for integration and provide rigorous theoretical guarantees for convergence rates, in both posterior mean and posterior contraction. The performance of probabilistic integrators is guaranteed to be no worse than non-probabilistic integrators and is, in many cases, asymptotically superior. These probabilistic integrators therefore enjoy the "best of both worlds", leveraging the sampling efficiency of advanced Monte Carlo methods whilst being equipped with valid probabilistic models for uncertainty quantification. Several applications and illustrations are provided, including examples from computer vision and system modelling using non-linear differential equations. A survey of open challenges in probabilistic integration is provided.},
	journal = {arXiv:1512.00933 [cs, math, stat]},
	author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Statistics Theory, preprint, Statistics - Computation, Statistics - Machine Learning},
	file = {1412/Briol et al. - 2015 - Probabilistic Integration.pdf}
}

@misc{osborne_epistemic_2008,
	title = {Epistemic {Uncertainty} in {Quantum} {Mechanics}},
	author = {Osborne, Michael A.},
	month = apr,
	year = {2008},
	keywords = {preprint},
	file = {1460/Osborne2008Epistemic_uncertainty_in_quantum_mechanics.pdf}
}

@article{fitzsimons_improved_2016,
	title = {Improved stochastic trace estimation using mutually unbiased bases},
	url = {http://arxiv.org/abs/1608.00117},
	abstract = {We examine the problem of estimating the trace of a matrix A when given access to an oracle which computes x† A x for an input vector x. We make use of the basis vectors from a set of mutually unbiased bases, widely studied in the field of quantum information processing, in the selection of probing vectors x. This approach offers a new state of the art single shot sampling variance while requiring only O(log(n)) random bits to generate each vector. This significantly improves on traditional methods such as Hutchinson's and Gaussian estimators in terms of the number of random bits required and worst case sample variance.},
	journal = {arXiv:1608.00117 [quant-ph]},
	author = {Fitzsimons, Jack K. and Osborne, Michael A. and Roberts, Stephen J. and Fitzsimons, Joe F.},
	month = jul,
	year = {2016},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, preprint, Quantum Physics},
	file = {1635/Fitzsimons et al. - 2016 - Improved stochastic trace estimation using mutuall.pdf}
}

@article{hutter_kernel_2013,
	title = {A {Kernel} for {Hierarchical} {Parameter} {Spaces}},
	url = {http://arxiv.org/abs/1310.5738},
	abstract = {We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite.},
	journal = {arXiv preprint arXiv:1310.5738},
	author = {Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	note = {00000},
	keywords = {preprint},
	file = {2152/1310.5738v1.pdf}
}

@article{swersky_raiders_2013,
	title = {Raiders of the lost architecture\_ {Kernels} for {Bayesian} optimization in conditional parameter spaces},
	abstract = {In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels.},
	journal = {NIPS workshop on Bayesian Optimization in theory and practice (BayesOpt’13)},
	author = {Swersky, Kevin and Duvenaud, David and Snoek, Jasper and Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {1173/hier-kern-workshop.pdf}
}

@techreport{osborne_gaussian_2007,
	title = {Gaussian processes for prediction},
	abstract = {We propose a powerful prediction algorithm built upon Gaussian processes (GPs). They are
particularly useful for their flexibility, facilitating accurate prediction even in the absence of strong
physical models.
GPs further allow us to work within a complete Bayesian probabilistic framework. As such, we
show how the hyperparameters of our system can be marginalised by use of Bayesian Monte Carlo, a
principled method of approximate integration. We employ the error bars of our GP’s predictions as
a means to select only the most informative data to store. This allows us to introduce an iterative
formulation of the GP to give a dynamic, on-line algorithm. We also show how our error bars can be
used to perform active data selection, allowing the GP to select where and when it should next take a
measurement.
We demonstrate how our methods can be applied to multi-sensor prediction problems where data
may be missing, delayed and/or correlated. In particular, we present a real network of weather sensors
as a testbed for our algorithm.},
	number = {PARG-07-01},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Osborne, Michael A. and Roberts, Stephen J.},
	year = {2007},
	keywords = {preprint},
	file = {1133/PARG-07-01.pdf}
}

@article{nickson_automated_2014,
	title = {Automated {Machine} {Learning} on {Big} {Data} using {Stochastic} {Algorithm} {Tuning}},
	url = {http://arxiv.org/abs/1407.7969},
	abstract = {We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.},
	journal = {arXiv preprint arXiv:1407.7969},
	author = {Nickson, Thomas and Osborne, Michael A. and Reece, Steven and Roberts, Stephen J.},
	year = {2014},
	note = {https://is.gd/e3JAVg},
	keywords = {preprint},
	file = {1051/1407.7969v1.pdf}
}

@article{salas_variational_2015,
	title = {A {Variational} {Bayesian} {State}-{Space} {Approach} to {Online} {Passive}-{Aggressive} {Regression}},
	url = {http://arxiv.org/abs/1509.02438},
	abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
	journal = {arXiv:1509.02438 [stat]},
	author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
	month = sep,
	year = {2015},
	keywords = {preprint},
	file = {1838/Salas et al. - 2015 - A Variational Bayesian State-Space Approach to Onl.pdf}
}

@article{rizvi_novel_2017,
	title = {A {Novel} {Approach} to {Forecasting} {Financial} {Volatility} with {Gaussian} {Process} {Envelopes}},
	url = {https://arxiv.org/abs/1705.00891},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1705.00891},
	author = {Rizvi, Syed Ali Asad and Roberts, Stephen J. and Osborne, Michael A. and Nyikosa, Favour},
	year = {2017},
	file = {2301/Rizvi et al. - 2017 - A Novel Approach to Forecasting Financial Volatili.pdf}
}

@article{fitzsimons_entropic_2017,
	title = {Entropic {Trace} {Estimates} for {Log} {Determinants}},
	url = {https://arxiv.org/abs/1704.07223},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1704.07223},
	author = {Fitzsimons, Jack and Granziol, Diego and Cutajar, Kurt and Osborne, Michael and Filippone, Maurizio and Roberts, Stephen},
	year = {2017},
	file = {2304/Fitzsimons et al. - 2017 - Entropic Trace Estimates for Log Determinants.pdf}
}